# reproduce f from knn or sigmoid

# -*- coding: utf-8 -*-
import os
from sklearn.metrics import f1_score
"""ppi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i8xRqB1bvfLoTvICrMtHFuvKqdvU5x6D
"""

# !pip install dgl-cu101

import dgl
from dgl.data.ppi import PPIDataset

from dgl.data.ppi import LegacyPPIDataset
from torch.utils.data import DataLoader
import torch
import numpy as np
import torch
import torch.nn.functional as F
import torch.nn as nn
import sys
import argparse 
from transfers.utils import gen_graph, generate_graph
from types import SimpleNamespace
from sage import SAGECompletedGraph

args = {
    "lam": 1.1,
    "mu": 100,
    "p": 8,
    "n": 8,
    "seed": 101,
    "kind": "sigmoid",
    "k": 4,
    "n_graphs": 4,
    "epochs": 1000,
    "batch_size": 2,
    "beta": 5,
    "alpha": 0.0,
}
args = SimpleNamespace(**args)
np.random.seed(args.seed)
torch.manual_seed(args.seed)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def init_weights(m):
    if type(m) == nn.Linear or type(m) == nn.Conv1d:
        torch.nn.init.xavier_uniform_(m.weight)
        m.bias.data.fill_(0.01)

class GNN(torch.nn.Module):
    def __init__(self,
                 in_channels,
                 hidden_channels,
                 num_classes,
                 normalize=False,
                 add_loop=True):
        super(GNN, self).__init__()
        self.add_loop = add_loop
        self.conv1 = SAGECompletedGraph(in_channels, hidden_channels, normalize)
        self.conv2 = SAGECompletedGraph(hidden_channels, num_classes, normalize)

    def forward(self, x, adj, mask=None):
        batch_size, num_nodes, in_channels = x.size()
        x = F.relu(self.conv1(x, adj, mask, self.add_loop))
        x = F.relu(self.conv2(x, adj, mask, self.add_loop))
        return F.log_softmax(x, dim=1)

class ModelSigmoid(nn.Module):
    def __init__(self):
        super().__init__()
        d = args.n*(args.n-1)//2
        self.d = d
        self.W = nn.Sequential(
            nn.BatchNorm1d(d),
            nn.Linear(d, d*2),
            nn.ReLU(),
            nn.Linear(d*2, d)
        )
        self.W.apply(init_weights)
    def forward_shuffle(self, Xs, halfAs):
        inds = np.random.permutation(len(Xs))
        Xs = [Xs[x] for x in inds]
        halfAs = [halfAs[x] for x in inds]
        return self.forward(Xs), torch.stack(halfAs,dim=0)
    def forward(self, Xs):
        xs = [torch.FloatTensor(x).to(device) for x in Xs]
        xs = [torch.pdist(x) for x in xs]
        xs = torch.stack(xs, dim=0)
        xs = self.W(xs)
        return xs
    def predict_adj(self, Xs):
        xs = self.forward(Xs)
        xs = torch.sigmoid(xs)
        xs[xs>=0.5] = 1
        xs[xs<0.5] = 0
        return xs
        
def weighted_cross_entropy_with_logits(logits, targets, beta):
    l=logits
    t=targets
    loss= -(beta*t*torch.log(torch.sigmoid(l)+1e-10) + (1-beta)*(1-t)*torch.log(torch.sigmoid(1-l)+1e-10))
    return loss

graphs = []
train_size = args.n_graphs - 2
u = np.random.multivariate_normal(np.zeros((args.p)), np.eye(args.p)/args.p, 1)
model_frozen = ModelSigmoid().to(device) # function F
model_frozen.requires_grad = False
model_frozen.eval()
for i in range(args.n_graphs):
    if i%100 == 0: print(f"{i+1}/{args.n_graphs}")
    Asyn, X, L = gen_graph(n=args.n, p=args.p, lam=args.lam, mu=args.mu, u=u)

    Asig = generate_graph(torch.FloatTensor(X), kind=args.kind, k=args.k)
    inds = torch.triu(torch.ones(len(Asig),len(Asig))) 
    inds[np.arange(len(Asig)), np.arange(len(Asig))] = 0
    Asig = torch.FloatTensor(Asig[inds == 1]).to(device)
    
#     with torch.no_grad():
#         Asig = model_frozen.predict_adj([X])[0]
    graphs.append((Asig, X, L, Asyn))
train_graphs = graphs[:train_size]
test_graphs = graphs[train_size:]

for graph in train_graphs[:10]:
    print(graph[0].sum() / len(graph[0]))

def compute_f1(pA, A, absolute=False):
    pA = pA.detach().cpu().numpy().astype(np.int)
    A = A.cpu().numpy().astype(np.int)
    if absolute:
        pA[pA >= 0.5] = 1
        pA[pA < 0.5] = 0
    f1 = f1_score(A, pA, average="macro")
    return f1

def f1_gnn(ypred, ytrue):
    ypred = ypred.detach().cpu().numpy().astype(np.int)
    ypred = np.argmax(ypred, axis=1)
    ytrue = ytrue.cpu().numpy().astype(np.int)
    f1 = f1_score(ytrue, ypred, average="micro")
    return f1


def batch_pdist_to_adjs(batch_halfAs):
    batch_adjs = torch.zeros((len(batch_halfAs), args.n, args.n))
    for i,halfA in enumerate(batch_halfAs):
        inds = torch.triu(torch.ones(args.n, args.n)) 
        inds[torch.arange(args.n), torch.arange(args.n)] = 0
        batch_adjs[i][inds == 1] = halfA
        batch_adjs[i] = batch_adjs[i].T + batch_adjs[i]
    return batch_adjs

model = ModelSigmoid().to(device) # another model that learns function F
model_gnn = GNN(args.p, args.p*2, 2).to(device)
print(model)
optim = torch.optim.Adam(model.parameters(), lr=0.01) # weight_decay=5e-4
loss_fn = nn.BCEWithLogitsLoss()

halfAs = [x[0] for x in graphs]
train_halfAs = halfAs[:train_size]
test_halfAs = halfAs[train_size:]

for iter in range(10000):
    model.train()
    optim.zero_grad()
    inds = np.random.permutation(len(train_graphs))[:args.batch_size]
    batch_xs = [train_graphs[x][1] for x in inds]
    batch_halfAs = [train_halfAs[x] for x in inds]
    batch_ys = torch.LongTensor([train_graphs[x][2] for x in inds]).to(device)

    pred_As, batch_halfAs = model.forward_shuffle(batch_xs, batch_halfAs)
    batch_xs = torch.FloatTensor(batch_xs).to(device)
    batch_adjs = batch_pdist_to_adjs(batch_halfAs)
    # A to graphsage
    outputs = model_gnn(batch_xs, batch_adjs)
    loss_gnn = F.nll_loss(outputs.view(-1, 2), batch_ys.view(-1))
    assert torch.isnan(pred_As).sum() == 0 or torch.isinf(pred_As).sum() == 0, "predAs nan inf"
#     loss = weighted_cross_entropy_with_logits(pred_As, batch_halfAs, beta=0.5).mean()
    loss_f = loss_fn(pred_As, batch_halfAs)
    loss = loss_f + loss_gnn*args.alpha
    assert not torch.isnan(loss) or not torch.isinf(loss), "loss nan"
    loss.backward()
    optim.step()
    
    if iter % 100 == 0:
        model.eval()
        with torch.no_grad():
            batch_xs = [x.cpu().numpy() for x in batch_xs[:5]] + [x[1] for x in test_graphs]
            batch_ys = [x.cpu().numpy() for x in batch_ys[:5]] + [x[2] for x in test_graphs]
            batch_halfAs = [x for x in batch_halfAs[:5]] + test_halfAs
            pred_As = model.forward(batch_xs)
            microfs = [compute_f1(predA, halfA) for predA, halfA in zip(pred_As, batch_halfAs)]
            microstr = " ".join([f"{f1:.2f}" for f1 in microfs])
            # acc gnn
            batch_xs = torch.FloatTensor(batch_xs).to(device)
            batch_ys = torch.LongTensor(batch_ys).to(device)
            batch_adjs = batch_pdist_to_adjs(batch_halfAs)
            outputs = model_gnn(batch_xs, batch_adjs)
            fs_gnn = [f1_gnn(output, y) for output,y in zip(outputs, batch_ys)]
            fs_gnn_str = " ".join([f"{f1:.2f}" for f1 in fs_gnn])
            print(f"Iter {iter} - loss {loss:.10f} - f1 {microstr} - gnn {fs_gnn_str}")

def save_graphs(A, X, L, outdir):
    print(f"\n==== Save graphs to {outdir}")
    edgelist = np.argwhere(A > 0)
    dataname = outdir.split("/")[-1]
    print(f"Number of edges: {A.sum()}")
    if not os.path.isdir(outdir):
        os.makedirs(outdir)
    with open(outdir + f"/{dataname}.txt", "w+") as fp:
        for src, trg in edgelist:
            fp.write(f"{src} {trg}\n")
    with open(outdir + "/labels.txt", "w+") as fp:
        for i, label in enumerate(L):
            fp.write(f"{i} {label}\n")
    np.savez_compressed(outdir + "/features.npz", features=X)
    print("=== Done ===\n")
def halfA_to_A(halfA):
    A = np.zeros((len(X), len(X)))
    inds = torch.triu(torch.ones(len(A),len(A))) 
    inds[np.arange(len(A)), np.arange(len(A))] = 0
    A[inds == 1] = halfA
    return A
# G(Atrain, Xtrain)
model.eval()
halfA, X, L, _ = train_graphs[0]
A = halfA_to_A(halfA.cpu().numpy())
save_graphs(A, X, L, f"data-transfers/synf-seed{args.seed}/Atrain-Xtrain-dat")

_, X, L, _ = test_graphs[0]
with torch.no_grad():
    halfA = model.predict_adj([X])[0]
A = halfA_to_A(halfA.cpu().numpy())
save_graphs(A, X, L, f"data-transfers/synf-seed{args.seed}/A2-Xtest-dat")
