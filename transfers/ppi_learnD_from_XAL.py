# -*- coding: utf-8 -*-
import os
from sklearn.metrics import f1_score
"""ppi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i8xRqB1bvfLoTvICrMtHFuvKqdvU5x6D
"""

# !pip install dgl-cu101

import dgl
from dgl.data.ppi import PPIDataset

from dgl.data.ppi import LegacyPPIDataset
from torch.utils.data import DataLoader
import torch
import numpy as np
import torch
import torch.nn.functional as F
import torch.nn as nn
import sys


def collate(sample):
    graphs, feats, labels = map(list, zip(*sample))
    graph = dgl.batch(graphs)
    feats = torch.from_numpy(np.concatenate(feats))
    labels = torch.from_numpy(np.concatenate(labels))
    return graph, feats, labels


train_dataset = LegacyPPIDataset(mode="test")
# train_dataloader = DataLoader(train_dataset, batch_size=1, collate_fn=collate)

# graphs = list(train_dataloader)
ids = np.random.permutation(len(train_dataset))
ids = [i for i in ids if train_dataset[i][0].number_of_nodes() < 15000000]
G1, F1, L1 = train_dataset[ids[0]]
G2, F2, L2 = train_dataset[ids[1]]
A1 = np.asarray(G1.adjacency_matrix_scipy().todense())
A2 = np.asarray(G2.adjacency_matrix_scipy().todense())
A1[A1 > 0] = 1
A2[A2 > 0] = 1
print(A1.shape, A2.shape)

L1 = train_dataset.test_labels[ids[0]]
L2 = train_dataset.test_labels[ids[1]]
pL1 = np.abs(L1.sum(axis=0)/len(L1) - 0.5)
pL2 = np.abs(L2.sum(axis=0)/len(L2) - 0.5)
print("Label percentage:")
print(pL1)
print(pL2)
scores = pL1*pL2
selected_label = np.argmin(scores)
print("Selected label: ", selected_label)
L1 = L1[:, selected_label]
L2 = L2[:, selected_label]

# n_nodes = 100
A1 = torch.FloatTensor(A1).cuda()
A2 = torch.FloatTensor(A2).cuda()
L1 = torch.LongTensor(L1).cuda()
L2 = torch.LongTensor(L2).cuda()
n_classes = 2
#


def compute_f1(pA, A):
    pA = pA.detach().cpu().numpy()
    pA[pA >= 0.5] = 1
    pA[pA < 0.5] = 0
    A = A.cpu().numpy()
    f1 = f1_score(A, pA, average="micro")
    return f1


def accuracy_classify(scores, ytrue):
    ypred = torch.argmax(scores, dim=1)
    return (ypred == ytrue).sum() / len(ypred)


class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.X1 = torch.FloatTensor(F1).cuda()
        self.X2 = torch.FloatTensor(F2).cuda()
        self.D1 = nn.Parameter(torch.empty(F1.shape).normal_(mean=0., std=1.)).cuda()
        self.D2 = nn.Parameter(torch.empty(F2.shape).normal_(mean=0., std=1.)).cuda()
        fdim = self.D1.shape[1]
        self.W = nn.Sequential(
            nn.Linear(fdim*2, fdim*4, bias=True),
            nn.ReLU(),
            nn.Linear(fdim*4, fdim*4, bias=True),
            nn.ReLU(),
            nn.Linear(fdim*4, fdim*2, bias=True),
        )
        self.classifyW = nn.Sequential(
            nn.Linear(fdim*2, fdim*4, bias=True),
            nn.ReLU(),
            nn.Linear(fdim*4, n_classes, bias=True)
        )

    def forward(self):
        D1 = self.W(torch.cat([self.D1, self.X1], dim=1))
        x1 = D1.mm(D1.t())
        D2 = self.W(torch.cat([self.D2, self.X2], dim=1))
        x2 = D2.mm(D2.t())
        score_classify1 = F.log_softmax(self.classifyW(
            torch.cat([self.D1, self.X1], dim=1)), dim=1)
        score_classify2 = F.log_softmax(self.classifyW(
            torch.cat([self.D2, self.X2], dim=1)), dim=1)
        return x1, x2, score_classify1, score_classify2


model = Model().cuda()
loss_fn = nn.MSELoss()
optim = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)
for iter in range(2000):
    model.train()
    optim.zero_grad()
    pred_A1, pred_A2, s1, s2 = model()
    loss = loss_fn(pred_A1, A1) + loss_fn(pred_A2, A2)
    loss += F.nll_loss(s1, L1) + F.nll_loss(s2, L2)
    loss.backward()
    optim.step()
    if iter % 50 == 0:
        microf11 = compute_f1(pred_A1, A1)
        microf12 = compute_f1(pred_A2, A2)
        acc1 = accuracy_classify(s1, L1)
        acc2 = accuracy_classify(s2, L2)
        print(
            f"Iter {iter} - loss {loss:.4f} - f1 {microf11:.3f}  {microf12:.3f} - accL {acc1:.3f} {acc2:.3f}")


# gen edgelist, labels, featuresh
print("Save graphs")
# gen edgelist, labels, featuresh
# X1 = model.X1.weight.detach().cpu().numpy()
# X2 = model.M(X1.t()).t().detach().cpu().numpy()
X1 = torch.cat([model.D1, model.X1], dim=1).detach().cpu().numpy()
X2 = torch.cat([model.D2, model.X2], dim=1).detach().cpu().numpy()
features = X1
edgelist = np.argwhere(A1.detach().cpu().numpy() > 0)
labels = L1.detach().cpu().numpy()

outdir = "data/ppi/0"
if not os.path.isdir(outdir):
    os.makedirs(outdir)

with open(outdir + "/edgelist.txt", "w+") as fp:
    for src, trg in edgelist:
        fp.write(f"{src} {trg}\n")
with open(outdir + "/labels.txt", "w+") as fp:
    for i, label in enumerate(labels):
        fp.write(f"{i} {label}\n")
np.savez_compressed(outdir + "/features.npz", features=features)

features = X2
edgelist = np.argwhere(A2.detach().cpu().numpy() > 0)
labels = L2.detach().cpu().numpy()

outdir = "data/ppi/1"
if not os.path.isdir(outdir):
    os.makedirs(outdir)

with open(outdir + "/edgelist.txt", "w+") as fp:
    for src, trg in edgelist:
        fp.write(f"{src} {trg}\n")
with open(outdir + "/labels.txt", "w+") as fp:
    for i, label in enumerate(labels):
        fp.write(f"{i} {label}\n")
np.savez_compressed(outdir + "/features.npz", features=features)

# python -u main.py --dataset temp/data-autoencoder/ppi/1/ --init ori --cuda graphsage --aggregator mean --load-model graphsage-best-model-0-ori-40.pkl > logs/ppi1-transfer-from-0.log
# python -u main.py --dataset temp/data-autoencoder/ppi/0/ --init ori --cuda graphsage --aggregator mean --load-model graphsage-best-model-1-ori-40.pkl > logs/ppi0-transfer-from-1.log


"""
for i in 0 1
do
echo $i
    python -u -W ignore main.py --dataset temp/data-autoencoder/ppi/$i --init ori --cuda gat > logs/ppi$i.log
done
python -u -W ignore main.py --dataset temp/data-autoencoder/ppi/1 --init ori --cuda gat --load-model gat-best-model-0-ori-40.pkl > logs/ppi1-tf-0.log
python -u -W ignore main.py --dataset temp/data-autoencoder/ppi/0 --init ori --cuda gat --load-model gat-best-model-1-ori-40.pkl > logs/ppi0-tf-1.log

"""

"""
for i in 0 1
do
echo $i
    python -u -W ignore main.py --dataset temp/data-autoencoder/ppi/$i --init ori --cuda graphsage --aggregator mean > logs/ppi$i.log
done
python -u -W ignore main.py --dataset temp/data-autoencoder/ppi/1 --init ori --cuda graphsage --aggregator mean --load-model graphsage-best-model-0-ori-40.pkl > logs/ppi1-tf-0.log
python -u -W ignore main.py --dataset temp/data-autoencoder/ppi/0 --init ori --cuda graphsage --aggregator mean --load-model graphsage-best-model-1-ori-40.pkl > logs/ppi0-tf-1.log

"""
