# reproduce f from knn or sigmoid

# -*- coding: utf-8 -*-
import os
from sklearn.metrics import f1_score
"""ppi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i8xRqB1bvfLoTvICrMtHFuvKqdvU5x6D
"""

# !pip install dgl-cu101

import dgl
from dgl.data.ppi import PPIDataset

from dgl.data.ppi import LegacyPPIDataset
from torch.utils.data import DataLoader
import torch
import numpy as np
import torch
import torch.nn.functional as F
import torch.nn as nn
import sys
import argparse 
import random
import string
from transfers.utils import gen_graph, generate_graph

parser = argparse.ArgumentParser()
parser.add_argument("--lam", type=float, default=1.0)
parser.add_argument("--mu", type=float, default=0)
parser.add_argument("--p", type=int, default=128)
parser.add_argument("--n", type=int, default=256)
parser.add_argument('--seed', type=int, default=100)
parser.add_argument('--kind', default='knn', help="choose from knn sigmoid")
parser.add_argument("--k", type=float, default=5)
parser.add_argument("--n_graphs", type=int, default=256)
parser.add_argument("--epochs", type=int, default=2000)
parser.add_argument("--batch-size", type=int, default=32)
parser.add_argument("--beta", type=float, default=0.4)
args = parser.parse_args()
np.random.seed(args.seed)
torch.manual_seed(args.seed)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

graphs = []
train_size = args.n_graphs - 5
u = np.random.multivariate_normal(np.zeros((args.p)), np.eye(args.p)/args.p, 1)
for i in range(args.n_graphs):
    Asyn, X, L = gen_graph(n=args.n, p=args.p, lam=args.lam, mu=args.mu, u=u)
    Asig = generate_graph(torch.FloatTensor(X), kind=args.kind, k=args.k)
    # onehot_ids = np.eye(len(X))
    # X = np.concatenate([X, onehot_ids], axis=1)
    graphs.append((Asig, X, L, Asyn))
train_graphs = graphs[:train_size]
test_graphs = graphs[train_size:]

def init_weights(m):
    if type(m) == nn.Linear or type(m) == nn.Conv1d:
        torch.nn.init.xavier_uniform_(m.weight)
        m.bias.data.fill_(0.01)

def f1_adj(pA, A):
    pA[pA >= 0.5] = 1
    pA[pA < 0.5] = 0
    f1 = f1_score(A, pA, average="macro")
    return f1

def f1_gnn(ypred, ytrue):
    ypred = ypred.detach().cpu().numpy().astype(np.int)
    ypred = np.argmax(ypred, axis=1)
    ytrue = ytrue.cpu().numpy().astype(np.int)
    f1 = f1_score(ytrue, ypred, average="micro")
    return f1

def batch_pdist_to_adjs(batch_halfAs):
    batch_adjs = torch.zeros((len(batch_halfAs), args.n, args.n)).to(device)
    for i,halfA in enumerate(batch_halfAs):
        inds = torch.triu(torch.ones(args.n, args.n)) 
        inds[torch.arange(args.n), torch.arange(args.n)] = 0
        batch_adjs[i][inds == 1] = halfA
        batch_adjs[i] = batch_adjs[i].T + batch_adjs[i]
    return batch_adjs

def halfA_to_A(halfA):
    A = np.zeros((len(X), len(X)))
    inds = torch.triu(torch.ones(len(A),len(A))) 
    inds[np.arange(len(A)), np.arange(len(A))] = 0
    A[inds == 1] = halfA
    A = A + A.T
    A[np.arange(len(A)), np.arange(len(A))] = 1
    return A

def A_to_halfA(A):
    inds = torch.triu(torch.ones(len(A),len(A))) 
    inds[np.arange(len(A)), np.arange(len(A))] = 0
    halfA = A[inds == 1]
    return halfA

class ModelSigmoid(nn.Module):
    def __init__(self):
        super().__init__()
        d = args.n*(args.n-1)//2
        self.d = d
        name = ''.join([random.choice(string.ascii_letters + string.digits) for n in range(4)])
        layers = [
            # (f'{name}-dropout1', nn.Dropout(0.05)),
            (f'{name}-linear1', nn.Linear(d, d*2)),
            (f'{name}-relu1', nn.LeakyReLU(0.2)),
#             (f'{name}-linear2', nn.Linear(d*2, d*2)),
#             (f'{name}-relu2', nn.LeakyReLU(0.2)),
#             (f'{name}-linear3', nn.Linear(d*2, d*2)),
#             (f'{name}-relu3', nn.LeakyReLU(0.2)),
            (f'{name}-linear4', nn.Linear(d*2, d)),
        ]
        self.W = nn.Sequential()
        [self.W.add_module(n, l) for n, l in layers]
        self.W.apply(init_weights)
    def forward_shuffle(self, Xs, As):
        """
        Xs: list of numpy array
        As: list of numpy array
        """
        inds = [np.random.permutation(args.n) for _ in range(len(Xs))]
        Xs = [X[x] for X, x in zip(Xs, inds)]
        As = [A[x][:,x] for A, x in zip(As,inds)]
        halfAs = [torch.FloatTensor(A_to_halfA(A)).to(device) for A in As]
        return self.forward(Xs), torch.stack(halfAs, dim=0)
    def forward(self, Xs):
        xs = [torch.FloatTensor(x).to(device) for x in Xs]
        xs = [torch.pdist(x) for x in xs]
        xs = torch.stack(xs, dim=0)
        xs = self.W(xs)
        xs = 1 - torch.sigmoid(xs) # lower the distance, higher probabilitity of edge
        return xs
    def predict_adj(self, Xs, absolute=True):
        xs = self.forward(Xs)
        if absolute:
            xs[xs < 0.5] = 0
            xs[xs >= 0.5] = 1
        return xs

class ModelKNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.Xs = [torch.FloatTensor(x).to(device) for _,x,_ in train_graphs]
        n_nodes, fdim = self.Xs[0].shape
        self.W0 = nn.Sequential(
            nn.Linear(fdim, fdim*2),
            nn.ReLU(),
            nn.Linear(fdim*2, fdim)
        )
        self.W = nn.Sequential(
            nn.Linear(n_nodes, n_nodes*2, bias=True),
            # nn.BatchNorm1d(n_nodes*2),
            nn.ReLU(),
            nn.Linear(n_nodes*2, n_nodes, bias=True),
            # nn.Softmax()
        )
        self.W2 = nn.Linear(n_nodes, n_nodes)

    def forward(self):
        xs = [F.normalize(x, dim=1) for x in self.Xs]
        # xs = [self.W0(x) for x in self.Xs]
        xs = [self.W(x.mm(x.t())) for x in xs]
        xs = [self.W2(F.log_softmax(x, dim=1)) for x in xs]
        xs = [F.sigmoid(x) for x in xs]
        return xs

def weighted_cross_entropy_with_logits(logits, targets, beta):
    l=logits
    t=targets
    loss= -(beta*t*torch.log(torch.sigmoid(l)+1e-10) + (1-beta)*(1-t)*torch.log(torch.sigmoid(1-l)+1e-10))
    return loss
if args.kind == "knn":
    model = ModelKNN().to(device)
else:
    model = ModelSigmoid().to(device)
print(model)
loss_fn = nn.BCELoss()
optim = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)
for iter in range(args.epochs):   
    model.train()
    optim.zero_grad()
    inds = np.random.permutation(len(train_graphs))[:args.batch_size]
    batch_xs = [train_graphs[x][1] for x in inds]
    batch_As = [train_graphs[x][0] for x in inds]
    for i in range(len(batch_xs)):
        shuffle_inds = np.random.permutation(args.n)
        batch_xs[i] = batch_xs[i][shuffle_inds]
        batch_As[i] = batch_As[i][shuffle_inds][:, shuffle_inds]
    pred_As = model.forward(batch_xs)
    batch_halfAs = torch.stack([torch.FloatTensor(A_to_halfA(A)).to(device) for A in batch_As], axis=0)
    loss = loss_fn(pred_As, batch_halfAs)
    loss.backward()
    optim.step()
    
    if iter % 100 == 0 or iter == 1:
        model.eval()
        with torch.no_grad():
            batch_xs = batch_xs[:5] + [x[1] for x in test_graphs]
            batch_halfAs = [x.cpu().numpy() for x in batch_halfAs[:5]] + [A_to_halfA(x[0]) for x in test_graphs]
            pred_As = model.predict_adj(batch_xs, absolute=True).cpu().numpy()
            microfs = [f1_adj(predA, halfA) for predA, halfA in zip(pred_As, batch_halfAs)]
            microstr = " ".join([f"{f1:.2f}" for f1 in microfs])
            print(f"Iter {iter} - loss {loss:.10f} - f1 {microstr}")

def save_graphs(A, X, L, outdir):
    print(f"\n==== Save graphs to {outdir}")
    edgelist = np.argwhere(A > 0)
    dataname = outdir.split("/")[-1]
    print(f"Number of edges: {A.sum()}")
    if not os.path.isdir(outdir):
        os.makedirs(outdir)
    with open(outdir + f"/{dataname}.txt", "w+") as fp:
        for src, trg in edgelist:
            fp.write(f"{src} {trg}\n")
    with open(outdir + "/labels.txt", "w+") as fp:
        for i, label in enumerate(L):
            fp.write(f"{i} {label}\n")
    np.savez_compressed(outdir + "/features.npz", features=X)
    print("=== Done ===\n")


# G(Atrain, Xtrain)
A, X, L, _ = train_graphs[0]
save_graphs(A, X, L, f"data-transfers/synf-seed{args.seed}/Atrain-Xtrain")

# G(Xtest, f(Xtest))
_, X, L, _ = test_graphs[0]
pdistA = model([X])[0].detach().cpu().numpy()
A = np.zeros((len(X), len(X)))
inds = torch.triu(torch.ones(len(A),len(A))) 
inds[np.arange(len(A)), np.arange(len(A))] = 0
A[inds == 1] = pdistA
A[np.arange(len(A)), np.arange(len(A))] = 1
A[A>=0.5] = 1
A[A<0.5] = 0
save_graphs(A, X, L, f"data-transfers/synf-seed{args.seed}/A2-Xtest")

# G(Xtest, KNN(Xtest))
_, X, L, _ = test_graphs[0]
A = generate_graph(torch.FloatTensor(X), kind="knn", k=args.k)
save_graphs(A, X, L, f"data-transfers/synf-seed{args.seed}/A3-Xtest")

# G(Xtest, sigmoid(Xtest))
A, X, L, _ = test_graphs[0]
# A = generate_graph(torch.FloatTensor(X), kind="sigmoid", k=args.k)
save_graphs(A, X, L, f"data-transfers/synf-seed{args.seed}/A4-Xtest")

# G(Xtest, Atest)
_, X, L, A = test_graphs[0]
save_graphs(A, X, L, f"data-transfers/synf-seed{args.seed}/Atest-Xtest")
