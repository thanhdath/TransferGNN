# reproduce f from knn or sigmoid

# -*- coding: utf-8 -*-
import os
from sklearn.metrics import f1_score
"""ppi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i8xRqB1bvfLoTvICrMtHFuvKqdvU5x6D
"""

# !pip install dgl-cu101

import dgl
from dgl.data.ppi import PPIDataset

from dgl.data.ppi import LegacyPPIDataset
from torch.utils.data import DataLoader
import torch
import numpy as np
import torch
import torch.nn.functional as F
import torch.nn as nn
import sys
import argparse 
from transfers.utils import gen_graph, generate_graph

parser = argparse.ArgumentParser()
parser.add_argument("--lam", type=float, default=1.0)
parser.add_argument("--mu", type=float, default=0)
parser.add_argument("--p", type=int, default=128)
parser.add_argument("--n", type=int, default=256)
parser.add_argument('--seed', type=int, default=100)
parser.add_argument('--kind', default='knn', help="choose from knn sigmoid")
parser.add_argument("--k", type=float, default=5)
parser.add_argument("--n_graphs", type=int, default=256)
parser.add_argument("--epochs", type=int, default=2000)
parser.add_argument("--batch-size", type=int, default=32)
parser.add_argument("--beta", type=float, default=0.4)
args = parser.parse_args()
np.random.seed(args.seed)
torch.manual_seed(args.seed)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

graphs = []
train_size = args.n_graphs - 2
u = np.random.multivariate_normal(np.zeros((args.p)), np.eye(args.p)/args.p, 1)
for i in range(args.n_graphs):
    Asyn, X, L = gen_graph(n=args.n, p=args.p, lam=args.lam, mu=args.mu, u=u)
    Asig = generate_graph(torch.FloatTensor(X), kind=args.kind, k=args.k)
    # onehot_ids = np.eye(len(X))
    # X = np.concatenate([X, onehot_ids], axis=1)
    graphs.append((Asig, X, L, Asyn))
train_graphs = graphs[:train_size]
test_graphs = graphs[train_size:]

def compute_f1(pA, A):
    pA = torch.sigmoid(pA)
    pA = pA.detach().cpu().numpy()
    pA[pA >= 0.5] = 1
    pA[pA < 0.5] = 0
    A = A.cpu().numpy()
    f1 = f1_score(A, pA, average="macro")
    return f1

# class ModelSigmoid(nn.Module):
#     def __init__(self):
#         super().__init__()
#         self.W = nn.Sequential(
#             nn.Linear(args.n*(args.n-1)//2, 128, bias=True),
#             # nn.ReLU(),
#             nn.ReLU(),
#             nn.Linear(128, 128),
#             nn.ReLU(),
#             nn.Linear(128, 128),
#             nn.ReLU(),
#             nn.Linear(128, args.n*(args.n-1)//2, bias=True)
#         )
#         # self.W = nn.Linear(args.)

#     def forward(self, graphs):
#         xs = [torch.FloatTensor(x).to(device) for _,x,_,_ in graphs]
#         xs = [torch.pdist(x) for x in xs]
#         # xs = [(x-x.mean())/x.std() for x in xs]
#         xs = [self.W(x) for x in xs]
#         # xs = [torch.sigmoid(x) for x in xs]
#         # xs = [(x - 0.5)*2 for x in xs]
#         return xs

class ModelSigmoid(nn.Module):
    def __init__(self):
        super().__init__()
        d = args.n*(args.n-1)//2
        self.d = d
        self.W = nn.Sequential(
            nn.Conv1d(1, d*2, d),
#             nn.ReLU(),
#             nn.Conv1d(d*2, d*4, d//2),
#             nn.ReLU(),
            nn.ConvTranspose1d(d*2, d, 1),
#             nn.ReLU(),
#             nn.ConvTranspose1d(d*2, d, 1)
#             nn.ReLU()
        )
    def forward_shuffle(self, graphs, halfAs):
        inds = np.random.permutation(len(graphs))
        graphs = [graphs[x] for x in inds]
        halfAs = [halfAs[x] for x in inds]
        return self.forward(graphs), torch.stack(halfAs,dim=0)
    def forward(self, graphs):
        xs = [torch.FloatTensor(x).to(device) for _,x,_,_ in graphs]
        xs = [torch.pdist(x) for x in xs]
        xs = torch.stack(xs, dim=0).view(-1, 1, self.d)
        xs = self.W(xs)
#         print(xs.shape)
        xs = xs.view(-1, self.d)
#         print(xs)
        return xs

class ModelKNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.Xs = [torch.FloatTensor(x).to(device) for _,x,_ in train_graphs]
        n_nodes, fdim = self.Xs[0].shape
        self.W0 = nn.Sequential(
            nn.Linear(fdim, fdim*2),
            nn.ReLU(),
            nn.Linear(fdim*2, fdim)
        )
        self.W = nn.Sequential(
            nn.Linear(n_nodes, n_nodes*2, bias=True),
            # nn.BatchNorm1d(n_nodes*2),
            nn.ReLU(),
            nn.Linear(n_nodes*2, n_nodes, bias=True),
            # nn.Softmax()
        )
        self.W2 = nn.Linear(n_nodes, n_nodes)

    def forward(self):
        xs = [F.normalize(x, dim=1) for x in self.Xs]
        # xs = [self.W0(x) for x in self.Xs]
        xs = [self.W(x.mm(x.t())) for x in xs]
        xs = [self.W2(F.log_softmax(x, dim=1)) for x in xs]
        xs = [F.sigmoid(x) for x in xs]
        return xs

def weighted_cross_entropy_with_logits(logits, targets, beta):
    l=logits
    t=targets
    loss= -(beta*t*torch.log(torch.sigmoid(l)+1e-10) + (1-beta)*(1-t)*torch.log(torch.sigmoid(1-l)+1e-10))
    return loss
if args.kind == "knn":
    model = ModelKNN().to(device)
else:
    model = ModelSigmoid().to(device)
# loss_fn = nn.MSELoss()
# loss_fn = nn.BCELoss()
# loss_fn = nn.BCEWithLogitsLoss()
optim = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)

halfAs = []
if args.kind == "sigmoid":
    for A,_,_,_ in graphs:
        inds = torch.triu(torch.ones(len(A),len(A))) 
        inds[np.arange(len(A)), np.arange(len(A))] = 0
        halfA = torch.FloatTensor(A[inds == 1]).to(device)
        halfAs.append(halfA)
elif args.kind == "knn":
    halfAs = []
    for A,_,_,_ in graphs:
        halfA = A.copy()
        # halfA[np.arange(len(A)), np.arange(len(A))] = 0
        halfAs.append(torch.FloatTensor(halfA).to(device))
train_halfAs = halfAs[:train_size]
test_halfAs = halfAs[train_size:]

for iter in range(args.epochs):
    model.train()
    optim.zero_grad()
    inds = np.random.permutation(len(train_graphs))[:args.batch_size]
    batch_graphs = [train_graphs[x] for x in inds]
    batch_halfAs = [train_halfAs[x] for x in inds]

    pred_As, batch_halfAs = model.forward_shuffle(batch_graphs, batch_halfAs)
    assert torch.isnan(pred_As).sum() == 0 or torch.isinf(pred_As).sum() == 0, "predAs nan inf"
    loss = weighted_cross_entropy_with_logits(pred_As, batch_halfAs, beta=args.beta).mean()
    assert not torch.isnan(loss) or not torch.isinf(loss),  "loss nan"
    loss.backward()
    optim.step()
    if iter % 100 == 0:
        microfs = [compute_f1(predA, halfA) for predA, halfA in zip(pred_As[:5], batch_halfAs[:5])]
        pred_As = model(test_graphs)
        print(pred_As[0])
        microfs += [compute_f1(predA, halfA) for predA, halfA in zip(pred_As, test_halfAs)]
        microstr = " ".join([f"{f1:.2f}" for f1 in microfs])
        print(f"Iter {iter} - loss {loss:.4f} - f1 {microstr}")

def save_graphs(A, X, L, outdir):
    print(f"\n==== Save graphs to {outdir}")
    edgelist = np.argwhere(A > 0)
    dataname = outdir.split("/")[-1]
    print(f"Number of edges: {A.sum()}")
    if not os.path.isdir(outdir):
        os.makedirs(outdir)
    with open(outdir + f"/{dataname}.txt", "w+") as fp:
        for src, trg in edgelist:
            fp.write(f"{src} {trg}\n")
    with open(outdir + "/labels.txt", "w+") as fp:
        for i, label in enumerate(L):
            fp.write(f"{i} {label}\n")
    np.savez_compressed(outdir + "/features.npz", features=X)
    print("=== Done ===\n")


# G(Atrain, Xtrain)
A, X, L, _ = train_graphs[0]
save_graphs(A, X, L, f"data-transfers/synf-seed{args.seed}/Atrain-Xtrain")
# G(Xtest, f(Xtest))

_, X, L, _ = test_graphs[0]
pdistA = model([test_graphs[0]])[0].detach().cpu().numpy()
A = np.zeros((len(X), len(X)))
inds = torch.triu(torch.ones(len(A),len(A))) 
inds[np.arange(len(A)), np.arange(len(A))] = 0
A[inds == 1] = pdistA
A[np.arange(len(A)), np.arange(len(A))] = 1
A[A>=0.5] = 1
A[A<0.5] = 0
save_graphs(A, X, L, f"data-transfers/synf-seed{args.seed}/A2-Xtest")

# G(Xtest, KNN(Xtest))
_, X, L, _ = test_graphs[0]
A = generate_graph(torch.FloatTensor(X), kind="knn", k=args.k)
save_graphs(A, X, L, f"data-transfers/synf-seed{args.seed}/A3-Xtest")

# G(Xtest, sigmoid(Xtest))
A, X, L, _ = test_graphs[0]
# A = generate_graph(torch.FloatTensor(X), kind="sigmoid", k=args.k)
save_graphs(A, X, L, f"data-transfers/synf-seed{args.seed}/A4-Xtest")

# G(Xtest, Atest)
_, X, L, A = test_graphs[0]
save_graphs(A, X, L, f"data-transfers/synf-seed{args.seed}/Atest-Xtest")
