# reproduce f from knn or sigmoid

# -*- coding: utf-8 -*-
import os
from sklearn.metrics import f1_score
"""ppi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i8xRqB1bvfLoTvICrMtHFuvKqdvU5x6D
"""

# !pip install dgl-cu101

import dgl
from dgl.data.ppi import PPIDataset

from dgl.data.ppi import LegacyPPIDataset
from torch.utils.data import DataLoader
import torch
import numpy as np
import torch
import torch.nn.functional as F
import torch.nn as nn
import sys
import argparse 
from transfers.utils import gen_graph, generate_graph
from types import SimpleNamespace
from sage import SAGECompletedGraph
import time
import random
import string

parser = argparse.ArgumentParser()
parser.add_argument('--lam', type=float, default=1.1)
parser.add_argument('--mu', type=int, default=100)
parser.add_argument('--p', type=int, default=8)
parser.add_argument('--n', type=int, default=32)
parser.add_argument('--seed', type=int, default=100)
parser.add_argument('--kind', type=str, default='sigmoid')
parser.add_argument('--k', type=int, default=5)
parser.add_argument('--n-graphs', type=int, default=128)
parser.add_argument('--epochs', type=int, default=5000)
parser.add_argument('--batch-size', type=int, default=8)
parser.add_argument('--beta', type=float, default=5)
parser.add_argument('--alpha', type=float, default=1.0)
args = parser.parse_args()

np.random.seed(args.seed)
torch.manual_seed(args.seed)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')

def init_weights(m):
    if type(m) == nn.Linear or type(m) == nn.Conv1d:
        torch.nn.init.xavier_uniform_(m.weight)
        m.bias.data.fill_(0.01)

class GNN(torch.nn.Module):
    def __init__(self,
                 in_channels,
                 hidden_channels,
                 num_classes,
                 normalize=False,
                 add_loop=True):
        super(GNN, self).__init__()
        self.add_loop = add_loop
        self.conv1 = SAGECompletedGraph(in_channels, hidden_channels, normalize)
        self.conv2 = SAGECompletedGraph(hidden_channels, num_classes, normalize)

    def forward(self, x, adj, mask=None):
        batch_size, num_nodes, in_channels = x.size()
        x = F.relu(self.conv1(x, adj, mask, self.add_loop))
        x = F.relu(self.conv2(x, adj, mask, self.add_loop))
        return F.log_softmax(x, dim=1)

class ModelSigmoid(nn.Module):
    def __init__(self):
        super().__init__()
        d = args.n*(args.n-1)//2
        self.d = d
        name = ''.join([random.choice(string.ascii_letters + string.digits) for n in range(4)])
        layers = [
            # (f'{name}-dropout1', nn.Dropout(0.05)),
            (f'{name}-linear1', nn.Linear(d, d*2)),
            (f'{name}-relu1', nn.ELU()),
#             (f'{name}-linear2', nn.Linear(d*2, d*2)),
#             (f'{name}-relu2', nn.LeakyReLU(0.2)),
#             (f'{name}-linear3', nn.Linear(d*2, d*2)),
#             (f'{name}-relu3', nn.LeakyReLU(0.2)),
            (f'{name}-linear4', nn.Linear(d*2, d)),
        ]
        self.W = nn.Sequential()
        [self.W.add_module(n, l) for n, l in layers]
        self.W.apply(init_weights)
    def forward_shuffle(self, Xs, As):
        """
        Xs: list of numpy array
        As: list of numpy array
        """
        inds = [np.random.permutation(args.n) for _ in range(len(Xs))]
        Xs = [X[x] for X, x in zip(Xs, inds)]
        As = [A[x][:,x] for A, x in zip(As,inds)]
        halfAs = [torch.FloatTensor(A_to_halfA(A)).to(device) for A in As]
        return self.forward(Xs), torch.stack(halfAs, dim=0)
    def forward(self, Xs):
        xs = [torch.FloatTensor(x).to(device) for x in Xs]
        xs = [torch.pdist(x) for x in xs]
        xs = torch.stack(xs, dim=0)
#         noise = torch.rand(xs.shape).to(device) / 100
#         xs += noise
        xs = self.W(xs)
        xs = 1 - torch.sigmoid(xs) # lower the distance, higher probabilitity of edge
        return xs
    def predict_adj(self, Xs, absolute=True):
        xs = self.forward(Xs)
        if absolute:
            xs[xs < 0.5] = 0
            xs[xs >= 0.5] = 1
        return xs

class ModelSigmoidScalable(nn.Module):
    def __init__(self):
        super().__init__()
        d = args.n*(args.n-1)//2
        self.d = d
        name = ''.join([random.choice(string.ascii_letters + string.digits) for n in range(4)])
        self.s = 127
#         split by half, self.d is even
        pre_inds = np.arange(0, self.d//2, self.d/self.s)
        post_inds = np.arange(self.d-1, self.d//2, -self.d/self.s)[::-1]
        if pre_inds.shape[0] + post_inds.shape[0] > self.s:
            inds = np.concatenate([pre_inds, post_inds[1:]], axis=-1)
        else:
            inds = np.concatenate([pre_inds, post_inds], axis=-1)
        self.sample_inds = np.rint(inds).astype(np.int)
        print(self.d, self.sample_inds)
        
        layers = [
            (f'{name}-bn1', nn.BatchNorm1d(1 + self.s)),
            (f'{name}-linear1', nn.Linear(1 + self.s, 128)),  # should reduce pdist size
            (f'{name}-relu1', nn.LeakyReLU(0.2)),
#             (f'{name}-linear2', nn.Linear(256, 128)),
#             (f'{name}-relu2', nn.LeakyReLU(0.2)),
            (f'{name}-linear3', nn.Linear(128, 2)),
            (f'{name}-softmax', nn.Softmax()),
        ]
        self.W = nn.Sequential()
        [self.W.add_module(n, l) for n, l in layers]
        self.W.apply(init_weights)
    def prepare_input(self, pdists, inds=None):
        """
        X: features of a graph, shape NxD
        """
#         sample pdists
        pdists_sorted = pdists.sort()[0]
        pdists_sample = pdists_sorted[self.sample_inds]
        if inds is not None:
            edges_score = pdists[inds]
            pdists_tiled = pdists_sample.repeat(1, len(inds)).view(-1, self.s)
        else:
            edges_score = pdists.view(-1, 1)
            pdists_tiled = pdists_sample.repeat(1, self.d).view(-1, self.s)
        inputs = torch.cat([edges_score, pdists_tiled], dim=1)
        return inputs
#     def forward(self, Xs):
#         xs = [torch.FloatTensor(x).to(device) for x in Xs]
#         pas = []
#         for x in xs:
#             pdists = torch.pdist(x).view(-1, 1)
#             inds = torch.arange(self.d)
#             preds = []
#             for ind in torch.split(inds, 256, dim=0):
#                 input = self.prepare_input(pdists, inds=ind)
#                 pred = self.forward_inputs(input)
#                 preds.append(pred)
#             preds = torch.stack(preds, dim=0).view(1, -1)
#             pas.append(preds)
#         pas = torch.stack(pas, dim=0).view(-1, self.d)
#         return pas
    def forward_inputs(self, inputs):
        preds = self.W(inputs)
        return preds
    def predict_adj(self, Xs, absolute=True, criterion=None, labels=None, shuffle=False):
        """
        X: shape NxD, features of a graph
        """
        xs = []
        loss = 0
        for i, X in enumerate(Xs):
            pdists = torch.pdist(torch.FloatTensor(X).to(device)).view(-1, 1)
            inds = torch.arange(len(pdists))
            if shuffle:
                inds = inds[torch.randperm(inds.size()[0])]
            preds = []
            for ind in torch.split(inds, 2048, dim=0):
                input = self.prepare_input(pdists, inds=ind)
                pred = self.forward_inputs(input).view(len(ind), 2)
                if criterion is not None and labels is not None:
                    label = labels[i][ind]
                    loss += criterion(pred, label)
                preds.append(pred)
            x = torch.cat(preds, dim=0)
            if absolute:
                x = x.argmax(dim=-1)
            xs.append(x)
        xs = torch.stack(xs, dim=0)
        return xs, loss

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        pdist_size = args.n*(args.n-1)//2
        layers = [
            nn.Dropout(0.1),
            nn.Linear(pdist_size, pdist_size*2),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.1),
            nn.Linear(pdist_size*2, pdist_size),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.1),
            nn.Linear(pdist_size, 1),
            nn.Sigmoid()
        ]
        self.layers = nn.Sequential(*layers)
    def forward(self, x):
        return self.layers(x).view(-1)
    
def weighted_cross_entropy_with_logits(logits, targets, beta):
    l=logits
    t=targets
    loss= -(beta*t*torch.log(torch.sigmoid(l)+1e-10) + (1-beta)*(1-t)*torch.log(torch.sigmoid(1-l)+1e-10))
    return loss

def f1_adj(pA, A):
    pA[pA >= 0.5] = 1
    pA[pA < 0.5] = 0
    f1 = f1_score(A, pA, average="macro")
    return f1

def f1_gnn(ypred, ytrue):
    ypred = ypred.detach().cpu().numpy().astype(np.int)
    ypred = np.argmax(ypred, axis=1)
    ytrue = ytrue.cpu().numpy().astype(np.int)
    f1 = f1_score(ytrue, ypred, average="micro")
    return f1

def batch_pdist_to_adjs(batch_halfAs):
    batch_adjs = torch.zeros((len(batch_halfAs), args.n, args.n)).to(device)
    for i,halfA in enumerate(batch_halfAs):
        inds = torch.triu(torch.ones(args.n, args.n)) 
        inds[torch.arange(args.n), torch.arange(args.n)] = 0
        batch_adjs[i][inds == 1] = halfA
        batch_adjs[i] = batch_adjs[i].T + batch_adjs[i]
    return batch_adjs

def halfA_to_A(halfA):
    A = np.zeros((len(X), len(X)))
    inds = torch.triu(torch.ones(len(A),len(A))) 
    inds[np.arange(len(A)), np.arange(len(A))] = 0
    A[inds == 1] = halfA
    A = A + A.T
    A[np.arange(len(A)), np.arange(len(A))] = 1
    return A

def A_to_halfA(A):
    inds = torch.triu(torch.ones(len(A),len(A))) 
    inds[np.arange(len(A)), np.arange(len(A))] = 0
    halfA = A[inds == 1]
    return halfA

graphs = []
train_size = args.n_graphs - 5
u = np.random.multivariate_normal(np.zeros((args.p)), np.eye(args.p)/args.p, 1)
print("u", u)
n_edges = []
for i in range(args.n_graphs):
    if i%100 == 0: print(f"{i+1}/{args.n_graphs}")
    Asbm, X, L = gen_graph(n=args.n, p=args.p, lam=args.lam, mu=args.mu, u=u)
    n_edges.append(Asbm.sum())
    graphs.append((None, X, L, Asbm))
train_graphs = graphs[:train_size]
test_graphs = graphs[train_size:]
print(f"Number of edges: min {np.min(n_edges)} - max {np.max(n_edges)} - ave {np.mean(n_edges):.2f}")
print("Auto adjust k")
ave_degree = np.mean(n_edges) / args.n
args.k = int(ave_degree)
print(f"Select k = {args.k}")


from itertools import chain
model1 = ModelSigmoidScalable().to(device) # function F
print(model1)
model_gnn = GNN(args.p, args.p*2, 2).to(device) # use to learn model1
print(model_gnn)
# optim = torch.optim.Adam(chain(model1.parameters(), model_gnn.parameters()), lr=0.01) # weight_decay=5e-4
discriminator = Discriminator().to(device)
print(discriminator)

optim = torch.optim.Adam(
    [
        {'params': model1.parameters(), 'lr': 5e-4}, #
        {'params': model_gnn.parameters()},
        {'params': discriminator.parameters(), 'lr': 5e-4}
    ],
    lr=0.01
)

"""
Do SBM gen adj một cách ngẫu nhiên tuân theo quy luật => không dùng loss BCE được
Tại sao cần dùng discriminator:  
=> train dùng thẳng BCE không được, cần 1 hàm loss khác. 
Vấn đề: làm sao để evaluate adj sinh ra là đúng hay sai.
Thêm discriminator: Transfer từ Agenbymodel-Xsbm sang Asbm-Xsbm ok
                    Transfer từ Asbm-Xsbm sang Agenbymodel-Xsbm not ok


Vấn đề: quá bị phụ thuộc vào kiến trúc model1 và learning rate mỗi model. Ví dụ:
- Thay đổi lr discriminator => gnn acc thấp = 0.5
- Tăng số layers của model1 => gnn acc thấp = 0.5, loss_distance = 0 ~ tất cả các graphs gen ra giống hệt nhau

Cần phải thay đổi các training gnn, ví dụ cần nhiều step để khẳng định có train gnn tốt hay ko thay vì 1 step như hiện tại
Nếu có thể gộp loss discriminator với loss distance, hiện tượng tất cả graphs giống nhau tương tự với mode collapse trong GAN 
"""

def loss_adj_fn(predA, A, dis_smooth=0.2):
    bs = len(predA)
    x = torch.cat([predA, A], dim=0)
    y = torch.FloatTensor(2 * bs).zero_().to(device)
    y[bs:] = dis_smooth # must classify as fake
    y[:bs] = 1 - dis_smooth # real
    preds = discriminator(x)
    loss = F.binary_cross_entropy(preds, y)
    return loss

def loss_reverse_distance(predAs):
#     predAs BxP
    return -torch.pdist(predAs).mean()

def loss_shuffle_features(xs):
    xs = [x.detach().cpu().numpy() for x in xs]
    inds = [np.random.permutation(args.n) for _ in range(len(xs))]
    xs_shuffle = [x[ind] for x, ind in zip(xs, inds)]
    pred_As = model1.predict_adj(xs, absolute=False)
    pred_As_shuffle = model1.predict_adj(xs_shuffle, absolute=False)
    pred_adjs = batch_pdist_to_adjs(pred_As)
    pred_adjs_shuffle = batch_pdist_to_adjs(pred_As_shuffle)
    
    pred_As_ = torch.stack([pred_adj[ind][:, ind] for pred_adj, ind in zip(pred_adjs, inds)])
    dists = torch.sqrt(torch.sum((pred_As_ - pred_adjs_shuffle)**2)).mean()
    return dists

# learn 1 graph mà acc cao nhất, tất cả các kiểu features đi qua model1 đều đưa ra output là graph này. 
# => làm sao để mỗi bộ feature cho ra 1 graph khác nhau
# shuffle features => still generate a particular graph => model1 do not care about the order of features
# shuffle features - should generate graph of equivalent nodes order.

# learn model1 | end2end model
loss_gnns = []
loss_distances = []
loss_shuffles = []
loss_adjs = []

# loss_adj_fn = nn.MSELoss()
for iter in range(args.epochs):
    model1.train()
    model_gnn.train()
    optim.zero_grad()
    inds = np.random.permutation(len(train_graphs))[:args.batch_size]
    batch_xs = [train_graphs[x][1] for x in inds]
    batch_ys = [train_graphs[x][2] for x in inds]
    batch_as = [train_graphs[x][3] for x in inds]
    for i in range(len(batch_xs)):
        shuffle_inds = np.random.permutation(args.n)
        batch_xs[i] = batch_xs[i][shuffle_inds]
        batch_ys[i] = batch_ys[i][shuffle_inds]
        batch_as[i] = batch_as[i][shuffle_inds][:, shuffle_inds]
    batch_ys = torch.LongTensor(batch_ys).to(device)
    pred_As = model1.predict_adj(batch_xs, absolute=False)
    batch_halfas = torch.stack([torch.FloatTensor(A_to_halfA(x)).to(device) for x in batch_as])
    loss_adj = loss_adj_fn(pred_As, batch_halfas)
    
    batch_xs = torch.FloatTensor(batch_xs).to(device)
#     pred_As to 
    pred_adjs = batch_pdist_to_adjs(pred_As)
    if iter%100 == 0:
        print(pred_As[0, :10])
    # A to graphsage
    outputs = model_gnn(batch_xs, pred_adjs)
    loss_gnn = F.nll_loss(outputs.view(-1, 2), batch_ys.view(-1))
    loss_distance = loss_reverse_distance(pred_As)
    loss_shuffle = loss_shuffle_features(batch_xs[:4])
    loss = loss_adj + loss_gnn + loss_distance # + loss_shuffle*0.1
#     loss = loss_gnn + loss_adj
    
    loss.backward()
    optim.step()
    
    loss_adjs.append(loss_adj.item())
    loss_gnns.append(loss_gnn.item())
    loss_distances.append(loss_distance.item())
    loss_shuffles.append(loss_shuffle.item())
    
#     evaluate
    if iter % 100 == 0:
        model1.eval()
        model_gnn.eval()
        with torch.no_grad():
            batch_xs = [x.cpu().numpy() for x in batch_xs[:5]] + [x[1] for x in test_graphs]
            batch_ys = [x.cpu().numpy() for x in batch_ys[:5]] + [x[2] for x in test_graphs]
            batch_halfas = [x.cpu().numpy() for x in batch_halfas[:5]] + [A_to_halfA(x[3]) for x in test_graphs]
            pred_As = model1.predict_adj(batch_xs, absolute=False)
#             print(pred_As[0,:10])
            # acc gnn
            batch_xs = torch.FloatTensor(batch_xs).to(device)
            batch_ys = torch.LongTensor(batch_ys).to(device)
            pred_adjs = batch_pdist_to_adjs(pred_As)
#             print(pred_adjs[0])
            outputs = model_gnn(batch_xs, pred_adjs)
            fs_gnn = [f1_gnn(output, y) for output,y in zip(outputs, batch_ys)]
            fs_gnn_str = " ".join([f"{f1:.2f}" for f1 in fs_gnn])
            
#             fs_adj = [f1_adj(pA.cpu().numpy(), halfA) for pA, halfA in zip(pred_As, batch_halfas)]
#             fs_adj_str = " ".join([f"{f1:.2f}" for f1 in fs_adj])
            print(f"Iter {iter} - loss_adj {loss_adj:.3f} loss_gnn {loss_gnn:.3f} - loss_dist {loss_distance:.3f} - loss_shuffle {loss_shuffle:.4f} - gnn {fs_gnn_str}")


# learn model1 done, frozen model1
for i in model1.parameters():
    model1.requires_grad = False
model1.eval()
# generate adj for all graphs
with torch.no_grad():
    print("u", u)
    for i in range(args.n_graphs):
        _, X, L, Asbm = graphs[i]
        halfA = model1.predict_adj([X], absolute=True)[0].cpu().numpy()
        A = halfA_to_A(halfA)
        graphs[i] = (A, X, L, Asbm)
train_graphs = graphs[:train_size]
test_graphs = graphs[train_size:]


def save_graphs(A, X, L, outdir):
    print(f"\n==== Save graphs to {outdir}")
    dataname = outdir.split("/")[-1]
    if not os.path.isdir(outdir):
        os.makedirs(outdir)
    np.savetxt(outdir + f"/{dataname}.txt", A, fmt="%.4f", delimiter=" ")
    with open(outdir + "/labels.txt", "w+") as fp:
        for i, label in enumerate(L):
            fp.write(f"{i} {label}\n")
    np.savez_compressed(outdir + "/features.npz", features=X)
    print("=== Done ===\n")

# G(Atrain, Xtrain), Atrain = Asbm, Xtrain = Xsbm
Af, X, L, Asbm = train_graphs[0]
print('Atrain-Xtrain')
save_graphs(Asbm, X, L, f"data-transfers/synf-sbm-seed{args.seed}/Atrain-Xtrain")

print('AtrainF-Xtrain')
save_graphs(Af, X, L, f"data-transfers/synf-sbm-seed{args.seed}/AtrainF-Xtrain")

print('A2-Xtest') # A2 = Af
Af, X, L, _ = test_graphs[0]
save_graphs(Af, X, L, f"data-transfers/synf-sbm-seed{args.seed}/A2-Xtest")

print('A3-Xtest') # A3 = KNN(X)
_, X, L, _ = test_graphs[0]
A3 = generate_graph(torch.FloatTensor(X), kind="knn", k=args.k)
save_graphs(A3, X, L, f"data-transfers/synf-sbm-seed{args.seed}/A3-Xtest")

print('A4-Xtest') # A4 = Sigmoid(X)
_, X, L, _ = test_graphs[0]
A4 = generate_graph(torch.FloatTensor(X), kind="sigmoid", k=args.k)
save_graphs(A4, X, L, f"data-transfers/synf-sbm-seed{args.seed}/A4-Xtest")

print('Atest-Xtest') # A4 = Sigmoid(X)
_, X, L, Asbm = test_graphs[0]
save_graphs(Asbm, X, L, f"data-transfers/synf-sbm-seed{args.seed}/Atest-Xtest")
