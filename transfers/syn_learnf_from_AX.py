# -*- coding: utf-8 -*-
import os
from sklearn.metrics import f1_score
"""ppi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i8xRqB1bvfLoTvICrMtHFuvKqdvU5x6D
"""

# !pip install dgl-cu101

import dgl
from dgl.data.ppi import PPIDataset

from dgl.data.ppi import LegacyPPIDataset
from torch.utils.data import DataLoader
import torch
import numpy as np
import torch
import torch.nn.functional as F
import torch.nn as nn
import sys
import argparse 
from transfers.utils import gen_graph

parser = argparse.ArgumentParser()
parser.add_argument("--lam", type=float, default=1.0)
parser.add_argument("--mu", type=float, default=0)
parser.add_argument("--p", type=int, default=8)
parser.add_argument("--n", type=int, default=256)
parser.add_argument('--seed', type=int, default=100)
args = parser.parse_args()
np.random.seed(args.seed)
torch.manual_seed(args.seed)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

graphs = []
for i in range(10):
    A, X, L = gen_graph(n=args.n, p=args.p, lam=args.lam, mu=args.mu)
    graphs.append((A, X, L))


def compute_f1(pA, A):
    pA = pA.detach().cpu().numpy()
    pA[pA >= 0.5] = 1
    pA[pA < 0.5] = 0
    A = A.cpu().numpy()
    f1 = f1_score(A, pA, average="micro")
    return f1

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.Xs = [
            torch.FloatTensor(x).to(device) for _,x,_ in graphs
        ]
        fdim = self.Xs[0].shape[1]
        self.W = nn.Sequential(
            nn.Linear(fdim, fdim*2, bias=True),
            nn.ReLU(),
            nn.Linear(fdim*2, fdim*2, bias=True),
            nn.ReLU(),
            nn.Linear(fdim*2, fdim, bias=True),
        )

    def forward(self):
        Ds = [self.W(x) for x in self.Xs]
        xs = [torch.pdist(d) for d in Ds]
        return xs


model = Model().to(device)
loss_fn = nn.MSELoss()
optim = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)

halfAs = []
for A,_,_ in graphs:
    inds = torch.triu(torch.ones(len(A),len(A))) 
    inds[np.arange(len(A)), np.arange(len(A))] = 0
    halfA = A[inds == 1]
    halfAs.append(halfA)

# for iter in range(1000):
#     model.train()
#     optim.zero_grad()
#     pred_As = model()
#     loss = 0
#     for pred_A, halfA in zip(pred_As, halfAs):
#         loss += loss_fn(pred_A, halfA)
#     loss.backward()
#     optim.step()
#     if iter % 50 == 0:
#         # microf11 = compute_f1(pred_As[0], halfAs[0])
#         # microf12 = compute_f1(pred_As[1], halfAs[1])
#         microfs = [compute_f1(pred_A,)]
#         print(f"Iter {iter} - loss {loss:.4f} - f1 {microf11:.3f}  {microf12:.3f}")

# gen edgelist, labels, featuresh
# print("Save graphs")
# X1 = F1
# X2 = F2
# features = X1
# edgelist = np.argwhere(A1.detach().cpu().numpy() > 0)
# labels = L1

# outdir = f"data-transfers/synD-seed{args.seed}/0"
# if not os.path.isdir(outdir):
#     os.makedirs(outdir)

# with open(outdir + f"/0.txt", "w+") as fp:
#     for src, trg in edgelist:
#         fp.write(f"{src} {trg}\n")
# with open(outdir + "/labels.txt", "w+") as fp:
#     for i, label in enumerate(labels):
#         fp.write(f"{i} {label}\n")
# np.savez_compressed(outdir + "/features.npz", features=features)

# features = X2
# edgelist = np.argwhere(A2.detach().cpu().numpy() > 0)
# labels = L2

# outdir = f"data-transfers/synD-seed{args.seed}/1"
# if not os.path.isdir(outdir):
#     os.makedirs(outdir)

# with open(outdir + f"/1.txt", "w+") as fp:
#     for src, trg in edgelist:
#         fp.write(f"{src} {trg}\n")
# with open(outdir + "/labels.txt", "w+") as fp:
#     for i, label in enumerate(labels):
#         fp.write(f"{i} {label}\n")
# np.savez_compressed(outdir + "/features.npz", features=features)

